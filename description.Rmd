# About this App

With this app we want to provide a short overview over different model selection techniques. You can try out how these techniques vary for different data sets and underlying structures. On the other tabs of the app, you have a lot of opportunities to try out all this stuff.  
Apart from the application of these techniques, we aim to give you a short overview on the techniques which are used in the app.

## AIC and BIC
The Akaike information criterion (AIC) and the Bayesian information criterion (BIC) are criteria for model selection. Both calculate an estimate of the model quality relative to other models. The model with the lowest AIC/BIC gets selected out of a set of models.  
The estimation is calculated by subtracting two times the log-Likelihood from a term depending on the complexity of the model. The AIC calculates the penalty term as two times the free parameters whereas the BIC multiplies the number of free parameters with the logarithm of the sample size.

### Formulas

$\text{AIC} = 2k - 2\ln(\hat L)$  

$\text{BIC} = \ln(n)k - 2\ln(\hat L)$

## Cross Validation
Cross Validation is a model validation technique that tries to calculate how good the model can predict data by predicting parts of the dataset by another part.
Therefore, the cross-validation method consists of two steps: At first you split your data into a training set and a testing set. After that you train your model on the training set and predict the testing set with the estimated model.  
The measure of interest in this procedure is the quality of the predictions the trained model makes. The quality of the model can for example be estimated by calculating the accuracy of predicting a datapoint (correctly). If your dependent variable is continuous space you can also take measures like mean squared error into account.
It is common to repeat this procedure so that every value of the dataset was predicted once.

<center>
![By Fabian Fl√∂ck (CC BY-SA 3.0), via Wikimedia Commons](images/cross_validation.jpg)
</center>
